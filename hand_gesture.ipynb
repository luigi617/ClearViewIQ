{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = [\n",
    "    'pointing',\n",
    "    'open_palm',\n",
    "    'thumb_index_touch',\n",
    "    'thumb_middle_touch',\n",
    "    'fist'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=128, num_layers=2):\n",
    "        super(GestureCNNLSTM, self).__init__()\n",
    "        \n",
    "        # CNN for spatial feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # Input: 3x224x224\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 32x112x112\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 64x56x56\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 128x28x28\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Output: 256x14x14\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of CNN output\n",
    "        self.feature_size = 256 * 14 * 14\n",
    "        \n",
    "        # LSTM for temporal feature extraction\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        \n",
    "        # Reshape to process each frame individually through CNN\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        x = self.cnn(x)  # Output: (batch_size * seq_len, 256, 14, 14)\n",
    "        x = x.view(batch_size, seq_len, -1)  # Output: (batch_size, seq_len, 256*14*14)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_size*2)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        last_time_step = lstm_out[:, -1, :]  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.fc(last_time_step)  # (batch_size, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=30, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the gesture subdirectories.\n",
    "            sequence_length (int): Number of frames per sequence.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.samples = self.make_dataset()\n",
    "    \n",
    "    def make_dataset(self):\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(self.root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            json_files = sorted([f for f in os.listdir(cls_dir) if f.endswith('.json')])\n",
    "            for i in range(len(json_files) - self.sequence_length + 1):\n",
    "                seq = json_files[i:i + self.sequence_length]\n",
    "                seq_paths = [os.path.join(cls_dir, f) for f in seq]\n",
    "                samples.append((seq_paths, self.class_to_idx[cls]))\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_paths, label = self.samples[idx]\n",
    "        sequence = []\n",
    "        for img_path in seq_paths:\n",
    "            with open(img_path, 'r') as f:\n",
    "                landmarks = json.load(f)\n",
    "                # Flatten landmarks: list of dicts to a flat list\n",
    "                flat_landmarks = []\n",
    "                for lm in landmarks:\n",
    "                    flat_landmarks.extend([lm['x'], lm['y'], lm['z']])\n",
    "                flat_landmarks = np.array(flat_landmarks, dtype=np.float32)\n",
    "                sequence.append(flat_landmarks)\n",
    "        \n",
    "        sequence = np.array(sequence)  # Shape: (sequence_length, 63)\n",
    "        if self.transform:\n",
    "            sequence = self.transform(sequence)\n",
    "        else:\n",
    "            # Normalize landmarks\n",
    "            sequence = (sequence - np.mean(sequence, axis=0)) / np.std(sequence, axis=0)\n",
    "            sequence = torch.FloatTensor(sequence)\n",
    "        \n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkTransform:\n",
    "    def __call__(self, sample):\n",
    "        # sample shape: (sequence_length, 63)\n",
    "        # Example: Add random Gaussian noise\n",
    "        noise = np.random.normal(0, 0.01, sample.shape)\n",
    "        sample = sample + noise\n",
    "        # Normalize\n",
    "        sample = (sample - np.mean(sample, axis=0)) / np.std(sample, axis=0)\n",
    "        return torch.FloatTensor(sample)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    LandmarkTransform()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset directories\n",
    "train_dir = 'gesture_dataset/train'\n",
    "val_dir = 'gesture_dataset/val'\n",
    "test_dir = 'gesture_dataset/test'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GestureDataset(root_dir=train_dir, sequence_length=30, transform=data_transforms)\n",
    "val_dataset = GestureDataset(root_dir=val_dir, sequence_length=30, transform=data_transforms)\n",
    "test_dataset = GestureDataset(root_dir=test_dir, sequence_length=30, transform=data_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of gesture classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Instantiate the model\n",
    "model = GestureCNNLSTM(num_classes=num_classes, hidden_size=128, num_layers=2)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for sequences, labels in loop:\n",
    "        sequences = sequences.to(device)  # Shape: (batch_size, seq_len, 63)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * sequences.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * sequences.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    val_epoch_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "          f'Train Loss: {epoch_loss:.4f} Train Acc: {epoch_acc:.2f}% '\n",
    "          f'Val Loss: {val_epoch_loss:.4f} Val Acc: {val_epoch_acc:.2f}%')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_epoch_acc > best_val_acc:\n",
    "        best_val_acc = val_epoch_acc\n",
    "        torch.save(model.state_dict(), 'best_gesture_model.pth')\n",
    "        print(f'Best model saved with Val Acc: {best_val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_gesture_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * sequences.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_epoch_loss = test_loss / len(test_dataset)\n",
    "test_epoch_acc = 100 * test_correct / test_total\n",
    "\n",
    "print(f'Test Loss: {test_epoch_loss:.4f} Test Acc: {test_epoch_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# After predictions on test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=gestures, yticklabels=gestures, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(all_labels, all_preds, target_names=gestures))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import os\n",
    "# import time\n",
    "# import json\n",
    "\n",
    "# # Initialize MediaPipe Hands\n",
    "# mp_hands = mp.solutions.hands\n",
    "# hands = mp_hands.Hands(\n",
    "#     static_image_mode=False,\n",
    "#     max_num_hands=1,\n",
    "#     min_detection_confidence=0.7,\n",
    "#     min_tracking_confidence=0.7\n",
    "# )\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Define gestures\n",
    "# gestures = [\n",
    "#     'swipe_up',\n",
    "#     'swipe_down',\n",
    "#     'swipe_left',\n",
    "#     'swipe_right',\n",
    "#     'pointing',\n",
    "#     'open_palm',\n",
    "#     'thumb_index_touch',\n",
    "#     'thumb_middle_touch',\n",
    "#     'fist'\n",
    "# ]\n",
    "\n",
    "# # Create directories for each gesture\n",
    "# data_dir = 'gesture_dataset'\n",
    "# for gesture in gestures:\n",
    "#     os.makedirs(os.path.join(data_dir, 'train', gesture), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(data_dir, 'val', gesture), exist_ok=True)\n",
    "\n",
    "# # Function to save landmarks\n",
    "# def save_landmarks(gesture, landmarks, seq_num, phase='train'):\n",
    "#     gesture_dir = os.path.join(data_dir, phase, gesture)\n",
    "#     filename = f'{gesture}_{seq_num}.json'\n",
    "#     filepath = os.path.join(gesture_dir, filename)\n",
    "#     with open(filepath, 'w') as f:\n",
    "#         json.dump(landmarks, f)\n",
    "\n",
    "# # Start video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# gesture = None\n",
    "# seq_num = {g: 0 for g in gestures}\n",
    "\n",
    "# print(\"Press the corresponding number key to start recording a gesture:\")\n",
    "# for idx, g in enumerate(gestures):\n",
    "#     print(f\"{idx}: {g}\")\n",
    "# print(\"Press 'q' to quit.\")\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Flip the frame for mirror effect\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = hands.process(image)\n",
    "\n",
    "#     # Draw hand landmarks\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "#     cv2.imshow('Data Collection', image)\n",
    "\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "#     elif key in [ord(str(i)) for i in range(len(gestures))]:\n",
    "#         gesture = gestures[key - ord('0')]\n",
    "#         print(f\"Recording gesture: {gesture}\")\n",
    "#         time.sleep(1)  # Brief pause before recording\n",
    "#         # Record for 1 second (assuming ~30 FPS)\n",
    "#         for _ in range(30):\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = cv2.flip(frame, 1)\n",
    "#             image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             results = hands.process(image)\n",
    "\n",
    "#             if results.multi_hand_landmarks:\n",
    "#                 for hand_landmarks in results.multi_hand_landmarks:\n",
    "#                     landmarks = []\n",
    "#                     for lm in hand_landmarks.landmark:\n",
    "#                         landmarks.append({\n",
    "#                             'x': lm.x,\n",
    "#                             'y': lm.y,\n",
    "#                             'z': lm.z\n",
    "#                         })\n",
    "#                     save_landmarks(gesture, landmarks, seq_num[gesture], phase='train')\n",
    "#                     seq_num[gesture] += 1\n",
    "\n",
    "#             # Optionally display progress\n",
    "#             cv2.imshow('Data Collection', frame)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "\n",
    "#         print(f\"Finished recording gesture: {gesture}\")\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torchvision import transforms\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the trained model\n",
    "model = GestureCNNLSTM(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('best_gesture_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Parameters\n",
    "sequence_length = 30\n",
    "buffer = deque(maxlen=sequence_length)\n",
    "transform = transforms.Compose([\n",
    "    LandmarkTransform()  # Assuming LandmarkTransform is defined as before\n",
    "])\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(3)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    gesture_label = None\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Extract and flatten landmarks\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            landmarks = np.array(landmarks, dtype=np.float32)\n",
    "            landmarks = (landmarks - landmarks.mean()) / (landmarks.std() + 1e-6)\n",
    "            landmarks = torch.FloatTensor(landmarks)\n",
    "\n",
    "            # Add to buffer\n",
    "            buffer.append(landmarks)\n",
    "\n",
    "            if len(buffer) == sequence_length:\n",
    "                # Prepare sequence\n",
    "                sequence = torch.stack(list(buffer)).unsqueeze(0).to(device)  # Shape: (1, seq_len, 63)\n",
    "\n",
    "                # Predict gesture\n",
    "                with torch.no_grad():\n",
    "                    output = model(sequence)\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    gesture_label = train_dataset.classes[predicted.item()]\n",
    "\n",
    "    # Display gesture label\n",
    "    if gesture_label:\n",
    "        cv2.putText(frame, f'Gesture: {gesture_label}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
