{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science: COMS W 4995 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal: Due November 5, 2024\n",
    "### Presentations: Due December 3 and 5, 2024\n",
    "### Final Report: Due December 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is one of the most important and, hopefully, exciting components of the course. You will have the opportunity to develop a deep learning system of your own choosing. \n",
    "You are free to select whatever framework (Pytorch, Tensorflow, etc.) you like, but you need create a report on your project in a Jupyter notebook. You are also free build on publically available models and code, but your report must clearly give attribution for the work of others and must clearly delineate your contributions. Also, half of the class will present their project during the last 2 days. All of the class will prepare videos of their presentation and submit these when the final report is due. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project description should include the title of the project, participants, a description of the objectives of the project, and a plan for how the project will be completed. The description of the objectives should include modest predictions of the success of the project. The plan for completion should include a description of the training data and how it will be obtained, a discussion of what deep learning framework will be used and why, and a rough description of the planned network architecture.\n",
    "\n",
    "You are permitted to work together on a project in groups of two or three, but group size must not exceed three participants.  For group projects there must be a clearly delineated division of labor: you should state in the project description and project report who was responsible for which portion of the project. Each student must hand in a separate report. (Students will not necessarily get the same grade for the same project.)\n",
    "\n",
    "You should mention whether you are simply re-implementing what others have done before but applying to new data or whether you are attempting to do something new to the best of your knowledge. Creative and original projects will be judged more kindly than those that are rehashing something in the existing literature. And projects that include a component in which data is acquired/curated into training and validation sets will be veiwed more favorably than those that simply download an existing data set such as ImageNet.\n",
    "\n",
    "As this is a computer vision course it is expected that your data will be visual, but exceptions might be made if the student is enthusiastic and persuasive enough. The most straightforward project would be to build a system that classifies images into categories. A more difficult project might be to build a system that detects and localizes a type of object within an image. A still more complicated project might involve joining a ConvNet/Vision Transformer with an LLM Transformer for a problem (like image captioning) that requires vision and language. But again, creative and original projects will be judged more kindly.  \n",
    "\n",
    "It is important to scope your project so that you get some working results. Project reports that say \"I tried this and this but nothing seemed to work...\" are discouraged. Above all, you should demonstrate end-to-end fluency in the basics of deep learning. \n",
    "\n",
    "I cannot wait to see the results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Project: RealGestureX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "participant: Luigi Liu (ll3840)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to develop an Integrated Static and Dynamic Hand Gesture Recognition System capable of real-time recognition of both static and dynamic hand gestures. By utilizing advanced hand tracking technologies and deep learning models, this project aims to create an intuitive interface for human-computer interaction, with applications in smart home controls, accessibility tools, and interactive gaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Primary Objective</strong>:\n",
    "\n",
    "To design and implement a gesture recognition system that accurately identifies a set of predefined static and dynamic hand gestures using PyTorch.\n",
    "\n",
    "<strong>Secondary Objectives</strong>:\n",
    "\n",
    "Develop a custom dataset encompassing both static and dynamic gestures to train and validate the model.<br>\n",
    "Integrate hand tracking using MediaPipe Hands to extract meaningful hand landmarks.<br>\n",
    "Optimize the model architecture to ensure real-time performance with high accuracy.<br>\n",
    "Deploy the system in a user-friendly interface that maps recognized gestures to specific commands.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Project Plan for Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gesture Selection: The system will recognize the following gestures:\n",
    "\n",
    "Static Gestures:\n",
    "\n",
    "Pointing,\n",
    "Open Palm,\n",
    "Thumb and Index Finger Touching,\n",
    "Thumb and Middle Finger Touching,\n",
    "Fist,\n",
    "\n",
    "Dynamic Gestures:\n",
    "\n",
    "Swipe Up,\n",
    "Swipe Down,\n",
    "Swipe Left,\n",
    "Swipe Right,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Creation: Due to the specificity of the required gestures, a custom dataset will be developed to ensure comprehensive coverage and high-quality samples. Each gesture sequence will be labeled accurately, with separate directories for training, validation, and testing sets to facilitate unbiased model evaluation.\n",
    "\n",
    "Tools and Frameworks:\n",
    "Camera Setup: Utilize high-resolution webcams to capture clear images and videos of hand gestures.\n",
    "Hand Tracking: Employ MediaPipe Hands to extract 21 3D hand landmarks per frame, providing detailed spatial information.\n",
    "\n",
    "Data Augmentation:\n",
    "\n",
    "Spatial Augmentation: Apply rotations, scaling, and translations to hand landmarks to simulate different hand orientations and positions.\n",
    "Temporal Augmentation: Vary the speed of gesture execution to capture different movement dynamics.\n",
    "Noise Injection: Introduce slight perturbations to landmark positions to enhance model robustness against real-world variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Network Architecture Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Model Architecture: CNN + LSTM\n",
    "\n",
    "Given the need to handle both static and dynamic gestures, a hybrid architecture combining Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory networks (LSTMs) for temporal sequence modeling is ideal.\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "CNN Layers:\n",
    "Purpose: Extract spatial features from each frame or hand landmark data.\n",
    "Structure: A series of convolutional, batch normalization, activation (ReLU), and pooling layers to progressively capture complex features.\n",
    "LSTM Layers:\n",
    "Purpose: Capture temporal dependencies and motion patterns across the sequence of frames.\n",
    "Structure: One or more LSTM layers (potentially bidirectional) to process the sequence data effectively.\n",
    "Fully Connected Layers:\n",
    "Purpose: Classify the extracted features into predefined gesture categories.\n",
    "Structure: Dense layers with dropout for regularization, culminating in a softmax layer for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Innovation and Originality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While gesture recognition systems utilizing CNN + LSTM architectures have been explored in existing research (e.g., the IEEE paper “Hand Gesture Recognition Using CNN and LSTM”), this project distinguishes itself through the following innovative and original approaches:\n",
    "\n",
    "Integration of Static and Dynamic Gesture Recognition:\n",
    "\n",
    "Unique Approach: Unlike many existing systems that focus solely on either static or dynamic gestures, this project simultaneously handles both within a single real-time framework. This integration allows for a more versatile interaction model, catering to a broader range of user inputs.\n",
    "Real-Time Performance: Emphasizing real-time processing ensures that both static and dynamic gestures are recognized and responded to instantaneously, enhancing user experience and system responsiveness.\n",
    "Custom Dataset Creation:\n",
    "\n",
    "Tailored Data: Instead of relying on existing datasets, the project involves creating a bespoke dataset that precisely matches the project's specific gesture requirements. This ensures higher accuracy and relevance in the recognition tasks.\n",
    "Data Diversity: By incorporating variations in participants, lighting conditions, and gesture execution styles, the dataset enhances the model's robustness and generalizability.\n",
    "Optimized Hybrid Architecture:\n",
    "\n",
    "Adaptive Modeling: The CNN + LSTM architecture is meticulously designed to effectively capture both spatial features from individual frames and temporal dynamics across sequences. This dual capability is crucial for accurately distinguishing between similar gestures that may vary in motion.\n",
    "Performance Enhancements: Implementing techniques like bidirectional LSTMs and attention mechanisms (if extended) can further refine the model's ability to focus on critical movement patterns, improving overall recognition accuracy.\n",
    "System Integration and Deployment:\n",
    "\n",
    "User-Friendly Interface: Beyond model development, the project emphasizes the integration of the recognition system into practical applications, mapping gestures to real-world commands. This end-to-end approach ensures that the system is not only theoretically sound but also practically applicable.\n",
    "Scalability: The system is designed with scalability in mind, allowing for easy addition of new gestures and functionalities without significant overhauls to the existing infrastructure.\n",
    "Comprehensive Evaluation and Testing:\n",
    "\n",
    "Real-World Testing: Conducting extensive user testing across different scenarios ensures that the system performs reliably under varied conditions, addressing potential real-world challenges that purely academic models might overlook.\n",
    "Performance Metrics: Utilizing a range of evaluation metrics, including confusion matrices and classification reports, provides a holistic understanding of the model's strengths and areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Expected Outcomes and Success Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Accuracy: Achieve an overall gesture recognition accuracy of 85% or higher across both static and dynamic gestures.\n",
    "Real-Time Performance: Ensure the system processes gestures with a latency of less than 200 milliseconds, facilitating smooth user interactions.<br>\n",
    "Robustness: Demonstrate the system's ability to accurately recognize gestures under varying lighting conditions, hand orientations, and across different users.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
