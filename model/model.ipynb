{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset/\n",
    "├── images/\n",
    "│   ├── img1.jpg\n",
    "│   ├── img2.jpg\n",
    "│   └── ...\n",
    "├── global_scores.csv\n",
    "└── local_masks/\n",
    "    ├── img1_mask.png\n",
    "    ├── img2_mask.png\n",
    "    └── ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luigiliu/Desktop/Columbia/DL for CV 4995/Final project/model\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'u2net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cwd)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mu2net\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mu2net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m U2NET  \u001b[38;5;66;03m# Ensure U^2-Net is correctly installed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Device configuration\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'u2net'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "import torch\n",
    "from u2net.u2net import U2NET  # Ensure U^2-Net is correctly installed\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the U^2-Net model\n",
    "sal_model = U2NET(3, 1).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "sal_model.load_state_dict(torch.load('u2net/u2net.pth', map_location=device))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "sal_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "def generate_saliency_mask(image_path, model, device):\n",
    "    \"\"\"\n",
    "    Generates a saliency mask for the given image using U^2-Net.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        model (torch.nn.Module): Pre-trained saliency detection model.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Saliency mask normalized between 0 and 1.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((320, 320)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        d1, d2, d3, d4, d5, d6, d7 = model(input_tensor)\n",
    "        pred = d1[:, 0, :, :]\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)  # Normalize to [0,1]\n",
    "        pred = np.uint8(pred * 255)\n",
    "        pred = Image.fromarray(pred).resize(image.size, resample=Image.BILINEAR)\n",
    "        pred = np.array(pred) / 255.0  # Normalize to [0,1]\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class KonIQ10kDataset(Dataset):\n",
    "    def __init__(self, images_dir, csv_file, saliency_model, device, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Path to images.\n",
    "            csv_file (str): Path to the CSV file with global scores.\n",
    "            saliency_model (torch.nn.Module): Pre-trained saliency detection model.\n",
    "            device (torch.device): Device to perform computation on.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.global_scores = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.saliency_model = saliency_model\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.global_scores)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename and global score\n",
    "        img_name = self.global_scores.iloc[idx, 0]\n",
    "        score = self.global_scores.iloc[idx, 1]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Generate saliency mask\n",
    "        saliency_mask = generate_saliency_mask(img_path, self.saliency_model, self.device)\n",
    "        saliency_mask = np.expand_dims(saliency_mask, axis=-1)  # Add channel dimension\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_np, mask=saliency_mask)\n",
    "            image = augmented['image']\n",
    "            saliency_mask = augmented['mask']\n",
    "        \n",
    "        # Convert mask to binary (threshold can be adjusted)\n",
    "        saliency_mask = (saliency_mask > 0.5).float()\n",
    "        \n",
    "        return image, torch.tensor(score, dtype=torch.float32), saliency_mask.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "def get_transforms(train=True):\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Rotate(limit=15, p=0.3),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "images_dir = 'koniq-10k/images/'  # Update with your actual path\n",
    "csv_file = 'koniq-10k/annotations/koniq-10k.csv'  # Update with your actual path\n",
    "batch_size = 16\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Initialize the dataset\n",
    "full_dataset = KonIQ10kDataset(images_dir, csv_file, saliency_model=sal_model, device=device, transform=get_transforms(train=True))\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EfficientNetIQA(nn.Module):\n",
    "    def __init__(self, efficientnet_version='efficientnet-b0', pretrained=True):\n",
    "        super(EfficientNetIQA, self).__init__()\n",
    "        # Load EfficientNet backbone\n",
    "        self.backbone = EfficientNet.from_pretrained(efficientnet_version) if pretrained else EfficientNet.from_name(efficientnet_version)\n",
    "        \n",
    "        # Remove the classification head\n",
    "        self.backbone._fc = nn.Identity()\n",
    "        self.backbone._avg_pooling = nn.Identity()\n",
    "        \n",
    "        # Global Quality Assessment Head\n",
    "        self.global_head = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1)  # Regression output\n",
    "        )\n",
    "        \n",
    "        # Local Quality Assessment Head\n",
    "        # We'll add convolutional layers to generate a quality map\n",
    "        self.local_head = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through EfficientNet backbone\n",
    "        features = self.backbone.extract_features(x)  # Shape: [B, 1280, H, W]\n",
    "        \n",
    "        # Global Quality\n",
    "        # Adaptive pooling to get a fixed-size feature vector\n",
    "        pooled = F.adaptive_avg_pool2d(features, (1,1)).view(features.size(0), -1)  # Shape: [B, 1280]\n",
    "        global_quality = self.global_head(pooled).squeeze(1)  # Shape: [B]\n",
    "        \n",
    "        # Local Quality\n",
    "        local_quality_map = self.local_head(features).squeeze(1)  # Shape: [B, H, W]\n",
    "        local_quality_map = F.interpolate(local_quality_map, size=x.size(2), mode='bilinear', align_corners=False)  # Resize to input size\n",
    "        \n",
    "        return global_quality, local_quality_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = EfficientNetIQA().to(device)\n",
    "\n",
    "# Define loss functions\n",
    "criterion_global = nn.MSELoss()\n",
    "criterion_local = nn.BCELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 30\n",
    "alpha = 1.0  # Weight for global loss\n",
    "beta = 1.0   # Weight for local loss\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "# Initialize GradScaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_global_loss = 0.0\n",
    "    running_local_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    for images, scores, masks in loop:\n",
    "        images = images.to(device)\n",
    "        scores = scores.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            # Forward pass\n",
    "            outputs_global, outputs_local = model(images)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_global = criterion_global(outputs_global, scores)\n",
    "            loss_local = criterion_local(outputs_local, masks)\n",
    "            loss = alpha * loss_global + beta * loss_local\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update running losses\n",
    "        running_loss += loss.item()\n",
    "        running_global_loss += loss_global.item()\n",
    "        running_local_loss += loss_local.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item(), global_loss=loss_global.item(), local_loss=loss_local.item())\n",
    "    \n",
    "    # Scheduler step\n",
    "    # scheduler.step()\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_global_loss = 0.0\n",
    "    val_local_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, scores, masks in valid_loader:\n",
    "            images = images.to(device)\n",
    "            scores = scores.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs_global, outputs_local = model(images)\n",
    "            loss_global = criterion_global(outputs_global, scores)\n",
    "            loss_local = criterion_local(outputs_local, masks)\n",
    "            loss = alpha * loss_global + beta * loss_local\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_global_loss += loss_global.item()\n",
    "            val_local_loss += loss_local.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_global_loss = running_global_loss / len(train_loader)\n",
    "    avg_train_local_loss = running_local_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    avg_val_global_loss = val_global_loss / len(valid_loader)\n",
    "    avg_val_local_loss = val_local_loss / len(valid_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Global: {avg_train_global_loss:.4f} | Local: {avg_train_local_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Global: {avg_val_global_loss:.4f} | Local: {avg_val_local_loss:.4f}\")\n",
    "    \n",
    "    # Optionally, save model checkpoints\n",
    "    torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = EfficientNetIQA().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, jaccard_score\n",
    "\n",
    "model.eval()\n",
    "all_scores = []\n",
    "all_preds_global = []\n",
    "all_targets_global = []\n",
    "all_preds_local = []\n",
    "all_targets_local = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, scores, masks in valid_loader:\n",
    "        images = images.to(device)\n",
    "        scores = scores.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        outputs_global, outputs_local = model(images)\n",
    "        \n",
    "        # Collect global scores\n",
    "        all_preds_global.extend(outputs_global.cpu().numpy())\n",
    "        all_targets_global.extend(scores.cpu().numpy())\n",
    "        \n",
    "        # Collect local masks\n",
    "        all_preds_local.extend(outputs_local.cpu().numpy())\n",
    "        all_targets_local.extend(masks.cpu().numpy())\n",
    "\n",
    "# Global Metrics\n",
    "mse = mean_squared_error(all_targets_global, all_preds_global)\n",
    "mae = mean_absolute_error(all_targets_global, all_preds_global)\n",
    "pearson_corr, _ = pearsonr(all_targets_global, all_preds_global)\n",
    "spearman_corr, _ = spearmanr(all_targets_global, all_preds_global)\n",
    "\n",
    "print(\"Global Quality Assessment Metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "# Local Metrics\n",
    "# Binarize predictions with a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "all_preds_local_bin = (np.array(all_preds_local) > threshold).astype(int)\n",
    "all_targets_local_bin = (np.array(all_targets_local) > 0.5).astype(int)\n",
    "\n",
    "# Flatten the masks for metric computation\n",
    "all_preds_local_bin_flat = all_preds_local_bin.flatten()\n",
    "all_targets_local_bin_flat = all_targets_local_bin.flatten()\n",
    "\n",
    "iou = jaccard_score(all_targets_local_bin_flat, all_preds_local_bin_flat)\n",
    "precision = precision_score(all_targets_local_bin_flat, all_preds_local_bin_flat)\n",
    "recall = recall_score(all_targets_local_bin_flat, all_preds_local_bin_flat)\n",
    "f1 = f1_score(all_targets_local_bin_flat, all_preds_local_bin_flat)\n",
    "auc = roc_auc_score(all_targets_local_bin_flat, np.array(all_preds_local))\n",
    "\n",
    "print(\"\\nLocal Quality Assessment Metrics:\")\n",
    "print(f\"IoU: {iou:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_quality_maps(model, dataset, device, num_samples=5):\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            image, score, mask = dataset[idx]\n",
    "            input_image = image.unsqueeze(0).to(device)\n",
    "            pred_global, pred_local = model(input_image)\n",
    "            pred_global = pred_global.item()\n",
    "            pred_local = pred_local.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Threshold the local quality map\n",
    "            pred_local_bin = (pred_local > 0.5).astype(int)\n",
    "            \n",
    "            # Original image\n",
    "            img = image.permute(1, 2, 0).cpu().numpy()\n",
    "            img = np.clip(img * np.array([0.229, 0.224, 0.225]) + \n",
    "                          np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "            \n",
    "            # Ground truth mask\n",
    "            gt_mask = mask.cpu().numpy()\n",
    "            \n",
    "            # Plotting\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "            axs[0].imshow(img)\n",
    "            axs[0].set_title(f\"Original Image\\nGlobal Score: {score:.2f}\")\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            axs[1].imshow(img)\n",
    "            axs[1].imshow(gt_mask, alpha=0.5, cmap='jet')\n",
    "            axs[1].set_title(\"Ground Truth Quality Map\")\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            axs[2].imshow(img)\n",
    "            axs[2].imshow(pred_local, alpha=0.5, cmap='jet')\n",
    "            axs[2].set_title(f\"Predicted Quality Map\\nGlobal Score: {pred_global:.2f}\")\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_heatmap(model, dataset, device, num_samples=5):\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            image, score, mask = dataset[idx]\n",
    "            input_image = image.unsqueeze(0).to(device)\n",
    "            pred_global, pred_local = model(input_image)\n",
    "            pred_global = pred_global.item()\n",
    "            pred_local = pred_local.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Normalize the quality map for better visualization\n",
    "            pred_local_norm = (pred_local - pred_local.min()) / (pred_local.max() - pred_local.min() + 1e-8)\n",
    "            \n",
    "            # Original image\n",
    "            img = image.permute(1, 2, 0).cpu().numpy()\n",
    "            img = np.clip(img * np.array([0.229, 0.224, 0.225]) + \n",
    "                          np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "            \n",
    "            # Plotting\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "            axs[0].imshow(img)\n",
    "            axs[0].set_title(f\"Original Image\\nGlobal Score: {score:.2f}\")\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            axs[1].imshow(img)\n",
    "            axs[1].imshow(pred_local_norm, alpha=0.6, cmap='jet')\n",
    "            axs[1].set_title(f\"Predicted Quality Heatmap\\nGlobal Score: {pred_global:.2f}\")\n",
    "            axs[1].axis('off')\n",
    "            \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Quality Maps\n",
    "visualize_quality_maps(model, full_dataset, device, num_samples=3)\n",
    "\n",
    "# Visualize Heatmaps\n",
    "visualize_heatmap(model, full_dataset, device, num_samples=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
